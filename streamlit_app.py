import streamlit as st
from raw_model.tongyi_llm import DashLLM
from rag_model import qa_chain


# ä½¿ç”¨ streamlit æ„å»ºå‰ç«¯é¡µé¢ï¼Œå®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.streamlit.io/develop/api-reference/write-magic


def llm_response(input_text, api_key):
    llm = DashLLM(api_key=api_key)
    answer = llm.invoke(input_text)
    return answer


def rag(input_text, api_key):
    return qa_chain.get_qa_chain(input_text, api_key)


if __name__ == "__main__":

    st.title('ğŸ¦œğŸ”— LLM åº”ç”¨ç¤ºä¾‹')
    api_key = st.sidebar.text_input('é€šä¹‰åƒé—®api-key', type='password')
    selected_method = st.radio(
        "ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ",
        ["æ™®é€šæ¨¡å¼", "é«˜çº§æ¨¡å¼"],
        captions=["æ™®é€šæ¨¡å‹ï¼Œä¸å¸¦å†å²è®°å½•", "ç‰¹å®šä¼˜åŒ–ï¼Œå¸¦å†å²è®°å½•"])
    if selected_method == 'æ™®é€šæ¨¡å¼':
        with st.form('my_form'):
            text = st.text_area('ğŸ¤“ä»Šå¤©æƒ³æé—®äº›ä»€ä¹ˆï¼Ÿ', placeholder='ä¸ºæˆ‘æ¨èä¸€é“æµ™æ±Ÿçš„å°åƒ ğŸ–')
            submitted = st.form_submit_button('âœˆï¸ï¸')
            if not api_key.startswith('sk-'):
                st.toast(
                    'è¯·å…ˆåœ¨å·¦ä¾§è¾“å…¥ä»¥\'sk-\'å¼€å¤´çš„é€šä¹‰å¯†é’¥ é»˜è®¤å€¼ï¼šsk-0a053959c62840ed94fd1a03324f41fc'
                    'å¯å‰å¾€ https://dashscope.console.aliyun.com/apiKey å¼€é€š~',
                    icon='âš ')
            if api_key.startswith('sk-') and submitted:
                if text == '':
                    st.toast("æé—®å†…å®¹ä¸å¯ä¸ºç©ºï¼")
                answer = llm_response(text, api_key)
                st.info(answer)
    else:  # è°ƒç”¨ RAG è¿›è¡Œå›ç­”
        if 'messages' not in st.session_state:
            st.session_state.messages = []
        messages = st.container(height=300)
        if prompt := st.chat_input("Say something"):
            # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
            st.session_state.messages.append({"role": "user", "text": prompt})
            # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”
            answer = rag(prompt, api_key)
            # æ£€æŸ¥å›ç­”æ˜¯å¦ä¸º None
            if answer is not None:
                # å°†LLMçš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
                st.session_state.messages.append({"role": "assistant", "text": answer})
            # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
            for message in st.session_state.messages:
                if message["role"] == "user":
                    messages.chat_message("user").write(message["text"])
                elif message["role"] == "assistant":
                    messages.chat_message("assistant").write(message["text"])

    st.page_link("https://github.com/Nanged23/LLM-Application", label="Github repo", icon="ğŸŒ", use_container_width=True)
    st.caption("è”ç³»ä½œè€…ï¼šferdinandlekae46@gmail.com")

# #ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾
# def get_qa_chain(question:str,api_key:str):
#     vectordb = get_vectordb()
#     llm = ChatOpenAI(model_name = "gpt-3.5-turbo", temperature = 0,api_key = api_key)
#     template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
#         æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
#         {context}
#         é—®é¢˜: {question}
#         """
#     QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context","question"],
#                                  template=template)
#     qa_chain = RetrievalQA.from_chain_type(llm,
#                                        retriever=vectordb.as_retriever(),
#                                        return_source_documents=True,
#                                        chain_type_kwargs={"prompt":QA_CHAIN_PROMPT})
#     result = qa_chain({"query": question})
#     return result["result"]
